<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<title>Slides</title>

<meta name="description" content="">
<meta name="author" content="">
<meta name="generator" content="reveal-ck 4.0.0">



<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">


<!-- Code syntax highlighting -->
<link rel="stylesheet" href="css/reset.css">
<link rel="stylesheet" href="css/reveal.css">
<link rel="stylesheet" href="css/theme/night.css" id="theme">

<!-- Theme used for syntax highlighting of code -->
<link rel="stylesheet" href="lib/css/monokai.css">

<link rel="stylesheet" href="css/reveal-ck.css">


<!-- Printing and PDF exports -->
<script>
  var link = document.createElement( 'link' );
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
  document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

  </head>

  <body>
    <div class="reveal">
  <div class="slides">
    <section>

<h1>YOLOv4</h1>

<p>YOLOv4: Optimal Speed and Accuracy of Object Detection</p>

<ul>
  <li>Date: 2020-04-26</li>
  <li>Speaker: Hiroshi Nishigami</li>
  <li>Links:
    <ul>
      <li>pdf: <a href="https://arxiv.org/pdf/2004.10934.pdf">https://arxiv.org/pdf/2004.10934.pdf</a></li>
      <li>abs: <a href="https://arxiv.org/abs/2004.10934">https://arxiv.org/abs/2004.10934</a></li>
      <li>github: <a href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></li>
    </ul>
  </li>
</ul>

</section>
<section>
<section>

<h2>はじめに</h2>

<ul>
  <li>yolov3 の著者 pjreddie (Joseph Redmon) が出した論文ではない</li>
  <li>pjreddie は CV の研究から引退。
    <ul>
      <li><a href="https://twitter.com/pjreddie/status/1230524770350817280?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1253524202528731136&amp;ref_url=https%3A%2F%2Fblog.seishin55.com%2Fentry%2F2020%2F05%2F16%2F183132">軍事利用やプライバシーの問題を無視できなくなったからだとか。</a></li>
    </ul>
  </li>
  <li>
<a href="https://github.com/AlexeyAB/darknet">この論文 yolov4 の first author は、 darknet を fork して開発を続けていた人。</a>
    <ul>
      <li><a href="https://twitter.com/alexeyab84/status/1264188352271613952">本人のコメント</a></li>
    </ul>
  </li>
</ul>

</section>
<section>

<h2>おさらい</h2>

<ul>
  <li>
<a href="https://pjreddie.com/darknet/yolo/">YOLO</a> とは
    <ul>
      <li>You only look once (YOLO): <a href="https://medium.com/@rajansharma9467/yolo-you-only-look-once-2afdba1f6a32">it looks for the image/frame only once and able to detect all the objects in the image/frame</a>
</li>
      <li>リアルタイム物体検出 (object detection) システム</li>
      <li>昨今の、object detection のスタンダードな手法</li>
    </ul>
  </li>
</ul>

</section>
<section>

<ul>
  <li>
<a href="https://pjreddie.com/darknet/">darknet</a> とは
    <ul>
      <li>C で書かれたオープンソースのニューラルネットワークのフレームワーク</li>
      <li>構築済み (学習済み) の yolo series が利用できる</li>
    </ul>
  </li>
</ul>

</section>
</section>

<section>
<section>

<h2>概要</h2>

<ul>
  <li>最先端のテクニック・手法を (ある程度の仮説を立てながら) 総当たりで実験し、良いものを採用するための実験を行った (for single GPU)</li>
  <li>性能が良かった組み合わせを採用して、YOLOv4 として提案</li>
  <li>既存の高速(高FPS)のアルゴリズムの中で、最も精度が良い手法</li>
  <li>YOLOv3 よりも精度が高く、EfficientDet よりも速い</li>
</ul>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-24-01-30-56.png" style="background:none; border:none; box-shadow:none;"></p>

</section>
<section>

<h3>AP: mean average precision (mAP)</h3>

<ul>
  <li><a href="http://cocodataset.org/#detection-eval">averaged across all 10 IoU thresholds and all 80 categories</a></li>
</ul>

</section>
</section>

<section>
<section>

<h2>検証内容</h2>

<ol>
  <li>Influence of different features on Classifier training</li>
  <li>Influence of different features on Detector training</li>
  <li>Influence of different backbones and pretrained weightings on Detector training</li>
  <li>Influence of different mini-batch size on Detector training</li>
</ol>

</section>
<section>

<h2>手法探索とチューニング</h2>

<ul>
  <li>モデルアーキテクチャ
    <ul>
      <li>backbone: 画像の特徴抽出の役割</li>
      <li>neck: backboneから受けた特徴マップをよしなに操作して、よりよい特徴量を生み出す</li>
      <li>head: クラス分類やbbox(物体を囲む四角形)の位置を予測する</li>
    </ul>
  </li>
</ul>

</section>
<section>

<ul>
  <li>Bag of freebies
    <ul>
      <li>学習上の工夫</li>
    </ul>
  </li>
  <li>Bag of specials
    <ul>
      <li>少ないコスト(推論時間や計算リソース)で大きな精度向上ができるもの</li>
    </ul>
  </li>
</ul>

</section>
<section>

<h3>物体検出器のアーキテクチャ</h3>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-23-20-45-48.png" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<ul>
  <li>backbone: 画像の特徴抽出の役割</li>
  <li>neck: backboneから受けた特徴マップをよしなに操作して、よりよい特徴量を生み出す</li>
  <li>head: クラス分類やbbox(物体を囲む四角形)の位置を予測する
    <ul>
      <li>1-stage: 直接的に予測を行う
        <ul>
          <li>YOLO系列やSSD</li>
          <li>速度重視</li>
        </ul>
      </li>
      <li>2-stage: 候補領域を出してから予測を行う
        <ul>
          <li>R-CNN系列</li>
          <li>精度重視</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

</section>
<section>

<h3>ネットワークアーキテクチャ</h3>

<p>Backbone: CSPDarknet53<br>
Neck: SPP、PAN<br>
Head: YOLOv3</p>

</section>
<section>

<h4>Backbone: CSPDarknet53</h4>

<ul>
  <li>
<a href="https://arxiv.org/pdf/1911.11929.pdf">CSPNet</a>
    <ul>
      <li>精度をあまり落とさずに、計算コストを省略するための手法</li>
    </ul>
  </li>
  <li>CSPNet で提案される機構を Darknet53 (YOLOv3で使われているbackbone) に導入</li>
</ul>

</section>
<section>

<h4>Neck: SPP、PAN</h4>

<ul>
  <li>
<a href="https://arxiv.org/abs/1406.4729">SPP (Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition)</a>
    <ul>
      <li>複数サイズの window でプーリングして特徴量を作り<br>
、受容野を広げることができる</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/1803.01534.pdf">PAN (Path Aggregation Network for Instance Segmentation)</a></li>
</ul>

</section>
<section>

<h3>学習上の工夫 (Bag of freebies)と精度改善上の工夫 (Bag of specials)</h3>

<ul>
  <li>活性化関数
    <ul>
      <li>Mish acrivation</li>
      <li>bboxのregression loss</li>
      <li>CIoU-loss</li>
      <li>DIoU-NMS</li>
    </ul>
  </li>
</ul>

</section>
<section>

<ul>
  <li>データオーグメンテーション
    <ul>
      <li>CutMix</li>
      <li>Mosaic data augmentation</li>
      <li>Self-Adversarial Training</li>
    </ul>
  </li>
</ul>

</section>
<section>

<ul>
  <li>正則化
    <ul>
      <li>DropBlock regularization</li>
    </ul>
  </li>
  <li>正規化
    <ul>
      <li>CmBN</li>
    </ul>
  </li>
  <li>その他
    <ul>
      <li>Optimal hyper parameters</li>
      <li>Cosine annealing scheduler</li>
      <li>Class label smoothing</li>
    </ul>
  </li>
</ul>

</section>
<section>

<h3>Additional improvements</h3>

<ul>
  <li>Mosaic, a new data augmentation method</li>
  <li>Self-Adversarial Training (SAT)</li>
  <li>Genetic algorithmを使ったハイパーパラメータのチューニング</li>
  <li>modified SAM, modified PAN, and Cross mini-Batch Normalization (CmBN)</li>
</ul>

<aside class="notes">

<p>Mosaic と SAT は著者らによって新しく提案された Data Augmentation の手法</p>

</aside>

</section>
<section>

<h4>Mosaic</h4>

<p>4 つの画像を混ぜる</p>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-22-52-21.png" width="70%" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<h4>SAT: Self-Adversarial Training (SAT)</h4>

<ul>
  <li>1度、network weights の代わりには元々の画像を更新 (self-adversarial attack)</li>
  <li>2度目、この修正された画像に対して通常の学習を行う</li>
</ul>

</section>
<section>

<h4>Modified SAM</h4>

<p>spatial-wise から pointwise attention へ修正</p>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-17-57.png" width="70%" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<h3>Modified PAN</h3>

<p><a href="https://arxiv.org/pdf/1803.01534.pdf">PAN (Path Aggregation Network for Instance Segmentation)</a></p>

<p>addition から concatenation に変更</p>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-24-12.png" width="70%" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<h4>Cross mini-Batch Normalization (CmBN)</h4>

<p><a href="https://arxiv.org/abs/2002.05712">Cross-Iteration Batch Normalization (CBN) (2020/02/13 on arXiv)</a> をベースにして、改良を加えた</p>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-03-47.png" width="70%" style="background:white; border:none; box-shadow:none;"></p>

</section>
</section>

<section>
<section>

<h3>検証結果</h3>

</section>
<section>

<h4>Influence of different features on Classifier training</h4>

<ul>
  <li>data augmentation
    <ul>
      <li>bilateral blurring, MixUp, CutMix and Mosaic</li>
    </ul>
  </li>
  <li>activations
    <ul>
      <li>Leaky-ReLU (as default), Swish, and Mish</li>
    </ul>
  </li>
</ul>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-47-04.png" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<ul>
  <li>
<a href="https://arxiv.org/abs/1908.08681">Mish (A Self Regularized Non-Monotonic Neural Activation Function)</a> は、昨年発表された連続な活性化関数 (2019年10月)</li>
</ul>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-26-00-04-10.png" width="40%" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-48-51.png" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-50-34.png" style="background:white; border:none; box-shadow:none;"></p>

<p>CutMix, Mosaic, Label Smoothing, Mish の効果が大きい</p>

</section>
<section>

<h4>Influence of different features on Detector training</h4>

<ul>
  <li>Loss algorithms for bounded box regression
    <ul>
      <li>GIoU, CIoU, DIoU, MSE</li>
    </ul>
  </li>
  <li>単純な bbox の MSE 誤差よりも、IoU ベースの損失関数 (<a href="https://arxiv.org/abs/1911.08287">CIoU-loss: Nov 2019</a>) の方が良い</li>
</ul>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-26-00-14-20.png" style="background:white; border:none; box-shadow:none;"></p>

</section>
</section>

<section>
<section>

<h3>最終的に採用された手法</h3>

</section>
<section>

<h4>YOLOv4 アーキテクチャ</h4>

<ul>
  <li>Backbone: CSPDarknet53</li>
  <li>Neck: SPP , PAN</li>
  <li>Head: YOLOv3</li>
</ul>

</section>
<section>

<h4>Bag of Freebies (BoF) for backbone</h4>

<p>(BoF: 学習時の手法)</p>

<ul>
  <li>CutMix and Mosaic data augmentation</li>
  <li>DropBlock regularization</li>
  <li>Class label smoothing</li>
</ul>

</section>
<section>

<h4>Bag of Specials (BoS) for backbone</h4>

<p>(BoS: 推論時のテクニック・追加モジュール)</p>

<ul>
  <li>Mish activation</li>
  <li>Cross-stage partial connections (CSP)</li>
  <li>Multiinput weighted residual connections (MiWRC)</li>
</ul>

</section>
<section>

<h4>Bag of Freebies (BoF) for detector</h4>

<ul>
  <li>CIoU-loss</li>
  <li>CmBN</li>
  <li>DropBlock regularization</li>
  <li>Mosaic data augmentation</li>
  <li>Self-Adversarial Training</li>
  <li>Eliminate grid sensitivity</li>
  <li>Using multiple anchors for a single ground<br>
truth</li>
  <li>Cosine annealing scheduler</li>
  <li>Optimal hyperparameters (genetic algorithm)</li>
  <li>Random training shapes</li>
</ul>

</section>
<section>

<h4>Bag of Specials (BoS) for detector</h4>

<ul>
  <li>Mish activation</li>
  <li>SPP-block</li>
  <li>SAM-block</li>
  <li>PAN path-aggregation block</li>
  <li>DIoU-NMS</li>
</ul>

</section>
</section>

<section>
<section>

<h2>議論、課題など</h2>

<ul>
  <li>detector の BoF を改善できる余地がある（future work）　</li>
  <li>FPSが70~130程度あるが、V100の強めのマシンであることには注意が必要（既存のモデルに対するパフォーマンスがよいことには変わりない）</li>
</ul>

</section>
<section>

<p>エッジ領域である、Jetson AGX XavierはFPSが割と小さく、TensorRTに変換及び量子化等の対応が必要</p>

<p>Jetson AGX Xavier上で32FPS程度</p>

<p>Ref: <a href="https://github.com/AlexeyAB/darknet/issues/5386#issuecomment-621169646">https://github.com/AlexeyAB/darknet/issues/5386#issuecomment-621169646</a>)</p>

</section>
<section>

<p>ちなみに</p>

<ul>
  <li>NVIDIA Tesla V100
    <ul>
      <li><a href="https://www.monotaro.com/p/3201/6094/?utm_medium=cpc&amp;utm_source=Adwords&amp;utm_campaign=246-833-4061_6466659573&amp;utm_content=96539050923&amp;utm_term=_419857551521__aud-368712506548:pla-879931900035&amp;gclid=CjwKCAjwk6P2BRAIEiwAfVJ0rI_BDVoK7CUtr7mubZ5uS0cs-s8fLxzahnQYFKn_7w2sdZ3LkJb0fxoCd0AQAvD_BwE">販売価格(税別): ¥2,990,000</a></li>
    </ul>
  </li>
</ul>

</section>
</section>

<section>
<section>

<h2>先行研究と比べて何がすごい？（新規性について）</h2>

<ul>
  <li>アーキテクチャ・手法が object detection 性能に与える影響を調査した</li>
  <li>既存の手法よりも良い手法を提案した (YOLOv4)
    <ul>
      <li>YOLOv3 と同程度に速く、より高い精度</li>
      <li>EfficientDet より速く、同程度の精度</li>
    </ul>
  </li>
  <li>速度重視で物体認識モデルを考えるのであれば、選択の筆頭候補ということになる</li>
</ul>

</section>
</section>

<section>
<section>

<h2>参考資料</h2>

<ul>
  <li>yolov4 の日本語解説資料
    <ul>
      <li><a href="https://www.slideshare.net/DeepLearningJP2016/dlyolov4-optimal-speed-and-accuracy-of-object-detection-234027228">[DL輪読会]YOLOv4: Optimal Speed and Accuracy of Object Detection</a></li>
      <li><a href="https://blog.seishin55.com/entry/2020/05/16/183132">物体認識モデルYOLOv3を軽く凌駕するYOLOv4の紹介 - ほろ酔い開発日誌</a></li>
    </ul>
  </li>
  <li><a href="https://towardsdatascience.com/yolo-v4-optimal-speed-accuracy-for-object-detection-79896ed47b50">英語解説記事</a></li>
  <li>yolov3 architecture: <a href="https://www.blog.dmprof.com/post/a-closer-look-at-yolov3">A Closer Look at YOLOv3</a>
</li>
</ul>

</section>
</section>

  </div>
</div>

<script src="js/reveal.js"></script>


<script>
  (function() {
  function extend( a, b ) {
    for(var i in b) {
      a[i] = b[i];
    }
  }
  var baseOptions = {
    transition: 'default',
    hash: true,
    dependencies: [
      { src: 'plugin/markdown/marked.js' },
      { src: 'plugin/markdown/markdown.js' },
      { src: 'plugin/highlight/highlight.js' },
      { src: 'plugin/notes/notes.js', async: true }
    ]
  };

  

  var configOptions = {"controls":true,"progress":true,"history":true,"center":true}
  var initializeOptions = {};
  extend(initializeOptions, baseOptions);
  extend(initializeOptions, configOptions);
  Reveal.initialize(initializeOptions);
})();

</script>

  </body>
</html>
