<section>

<h1>YOLOv4</h1>

<p>YOLOv4: Optimal Speed and Accuracy of Object Detection</p>

<ul>
  <li>Date: 2020-04-26</li>
  <li>Speaker: Hiroshi Nishigami</li>
  <li>Links:
    <ul>
      <li>pdf: <a href="https://arxiv.org/pdf/2004.10934.pdf">https://arxiv.org/pdf/2004.10934.pdf</a></li>
      <li>abs: <a href="https://arxiv.org/abs/2004.10934">https://arxiv.org/abs/2004.10934</a></li>
      <li>github: <a href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></li>
    </ul>
  </li>
</ul>

</section>
<section>
<section>

<h2>はじめに</h2>

<ul>
  <li>yolov3 の著者 pjreddie (Joseph Redmon) が出した論文ではない</li>
  <li>pjreddie は CV の研究から引退。
    <ul>
      <li><a href="https://twitter.com/pjreddie/status/1230524770350817280?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1253524202528731136&amp;ref_url=https%3A%2F%2Fblog.seishin55.com%2Fentry%2F2020%2F05%2F16%2F183132">軍事利用やプライバシーの問題を無視できなくなったからだとか。</a></li>
    </ul>
  </li>
  <li>
<a href="https://github.com/AlexeyAB/darknet">この論文 yolov4 の first author は、 darknet を fork して開発を続けていた人。</a>
    <ul>
      <li><a href="https://twitter.com/alexeyab84/status/1264188352271613952">本人のコメント</a></li>
    </ul>
  </li>
</ul>

</section>
<section>

<h2>おさらい</h2>

<ul>
  <li>
<a href="https://pjreddie.com/darknet/yolo/">YOLO</a> とは
    <ul>
      <li>You only look once (YOLO): <a href="https://medium.com/@rajansharma9467/yolo-you-only-look-once-2afdba1f6a32">it looks for the image/frame only once and able to detect all the objects in the image/frame</a>
</li>
      <li>リアルタイム物体検出 (object detection) システム</li>
      <li>昨今の、object detection のスタンダードな手法</li>
    </ul>
  </li>
</ul>

</section>
<section>

<ul>
  <li>
<a href="https://pjreddie.com/darknet/">darknet</a> とは
    <ul>
      <li>C で書かれたオープンソースのニューラルネットワークのフレームワーク</li>
      <li>構築済み (学習済み) の yolo series が利用できる</li>
    </ul>
  </li>
</ul>

</section>
</section>

<section>
<section>

<h2>概要</h2>

<ul>
  <li>最先端のテクニック・手法を (ある程度の仮説を立てながら) 総当たりで実験し、良いものを採用するための実験を行った (for single GPU)</li>
  <li>性能が良かった組み合わせを採用して、YOLOv4 として提案</li>
  <li>既存の高速(高FPS)のアルゴリズムの中で、最も精度が良い手法</li>
  <li>YOLOv3 よりも精度が高く、EfficientDet よりも速い</li>
</ul>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-24-01-30-56.png" style="background:none; border:none; box-shadow:none;"></p>

</section>
<section>

<h3>AP: mean average precision (mAP)</h3>

<ul>
  <li><a href="http://cocodataset.org/#detection-eval">averaged across all 10 IoU thresholds and all 80 categories</a></li>
</ul>

</section>
</section>

<section>
<section>

<h2>検証内容</h2>

<ol>
  <li>Influence of different features on Classifier training</li>
  <li>Influence of different features on Detector training</li>
  <li>Influence of different backbones and pretrained weightings on Detector training</li>
  <li>Influence of different mini-batch size on Detector training</li>
</ol>

</section>
<section>

<h2>手法探索とチューニング</h2>

<ul>
  <li>モデルアーキテクチャ
    <ul>
      <li>backbone: 画像の特徴抽出の役割</li>
      <li>neck: backboneから受けた特徴マップをよしなに操作して、よりよい特徴量を生み出す</li>
      <li>head: クラス分類やbbox(物体を囲む四角形)の位置を予測する</li>
    </ul>
  </li>
</ul>

</section>
<section>

<ul>
  <li>Bag of freebies
    <ul>
      <li>学習上の工夫</li>
    </ul>
  </li>
  <li>Bag of specials
    <ul>
      <li>少ないコスト(推論時間や計算リソース)で大きな精度向上ができるもの</li>
    </ul>
  </li>
</ul>

<aside class="notes">

<p>著者は、これらを BoF, BoS と命名し、区別して議論しているが、一般的に使われる用語ではないと思われる。</p>

</aside>

</section>
<section>

<h3>物体検出器のアーキテクチャ</h3>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-23-20-45-48.png" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<ul>
  <li>backbone: 画像の特徴抽出の役割</li>
  <li>neck: backboneから受けた特徴マップをよしなに操作して、よりよい特徴量を生み出す</li>
  <li>head: クラス分類やbbox(物体を囲む四角形)の位置を予測する
    <ul>
      <li>1-stage: 直接的に予測を行う
        <ul>
          <li>YOLO系列やSSD</li>
          <li>速度重視</li>
        </ul>
      </li>
      <li>2-stage: 候補領域を出してから予測を行う
        <ul>
          <li>R-CNN系列</li>
          <li>精度重視</li>
        </ul>
      </li>
      <li>今回は、1-stage のみに focus する</li>
    </ul>
  </li>
</ul>

</section>
<section>

<h3>ネットワークアーキテクチャ</h3>

<p>Backbone: CSPDarknet53<br>
Neck: SPP、PAN<br>
Head: YOLOv3</p>

</section>
<section>

<h4>Backbone: CSPDarknet53</h4>

<ul>
  <li>
<a href="https://arxiv.org/pdf/1911.11929.pdf">CSPNet</a>
    <ul>
      <li>精度をあまり落とさずに、計算コストを省略するための手法</li>
    </ul>
  </li>
  <li>CSPNet で提案される機構を Darknet53 (YOLOv3で使われているbackbone) に導入</li>
</ul>

</section>
<section>

<h4>Neck: FPN、PANet</h4>

<ul>
  <li>
<a href="https://arxiv.org/abs/1612.03144">FPN (Feature Pyramid Network)</a>, Bi-FPN
    <ul>
      <li>YOLOv3 は FPN を Neck として採用し、異なるスケールの特徴を backbone から抽出している</li>
      <li>複数サイズの window でプーリングして特徴量を作り<br>
、受容野を広げることができる</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/1803.01534.pdf">PAN (Path Aggregation Network for Instance Segmentation)</a></li>
</ul>

</section>
<section>

<h4>Head</h4>

<ul>
  <li>bounding box (bbox) の分類タスクを担うネットワーク</li>
  <li>output の例として、 bounding box の (x, y, h, w) と k 個のクラスの確率 + 1 (バッググランドの確率)</li>
  <li>YOLO は anchor-based な検出器で、anchor ごとに head network が適用される</li>
  <li>Single Shot Detector (SSD) や RetinaNet も anchor-based な検出器である</li>
</ul>

</section>
<section>

<h3>Bag of freebies</h3>

<p>推論コストを上げず、学習手法と学習コストのみ変更させ、精度を改善する手法</p>

<ul>
  <li>bbox の regression loss
    <ul>
      <li>IoU-loss</li>
      <li>GIoU-loss</li>
      <li>CIoU-loss</li>
    </ul>
  </li>
</ul>

</section>
<section>

<ul>
  <li>データオーグメンテーション
    <ul>
      <li>CutOut</li>
      <li>CutMix</li>
      <li>Mosaic data augmentation</li>
      <li>Self-Adversarial Training</li>
    </ul>
  </li>
  <li>正則化
    <ul>
      <li>DropOut, DropConnect and DropBlock</li>
    </ul>
  </li>
</ul>

</section>
<section>

<ul>
  <li>正規化
    <ul>
      <li>CmBN (cross mini-batch normalization) (後述)</li>
    </ul>
  </li>
  <li>その他
    <ul>
      <li>Optimal hyper parameters</li>
      <li>Cosine annealing scheduler</li>
      <li>Class label smoothing</li>
    </ul>
  </li>
</ul>

</section>
<section>

<h4>Data augmentation</h4>

<ul>
  <li>明るさ、彩度、コントラスト、ノイズを変更したり、画像の回転、トリミングなどの幾何学的歪みを導入する</li>
  <li>モデルの汎化性能を上げることができる</li>
  <li>例えば、<a href="https://arxiv.org/abs/1708.04552">CutOut</a> や <a href="https://arxiv.org/abs/1708.04896">Random Erase</a> はランダムに画像の領域をマスクして適当な値で埋める</li>
</ul>

</section>
<section>

<p>Random Erase</p>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-26-23-34-08.png" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<h4>正則化</h4>

<ul>
  <li>overf-itting を防ぐ</li>
</ul>

</section>
<section>

<h4>loss</h4>

<ul>
  <li>伝統的なものは平均二乗誤差 (MSE: Mean Squared error)</li>
  <li>
<a href="https://arxiv.org/abs/1608.01471">IoU loss</a>: 予測された bbox と ground truth の bbox の面積を考慮</li>
  <li>
<a href="https://arxiv.org/pdf/1902.09630v2.pdf">GIoU loss</a>: 面積だけでなく、bbox の形と回転を考慮</li>
  <li>CIoU loss: 中心間の距離とアスペクト比を考慮</li>
  <li>YOLOv4 では CIoU loss が使われている (他の手法より、収束が速く、精度が良かったため)</li>
</ul>

</section>
<section>

<h3>Bag of specials</h3>

<p>推論コストを少しだけ上げて、物体検知の精度を大幅に上げる手法</p>

<ul>
  <li>Improving receptive field
    <ul>
      <li>
<a href="https://arxiv.org/abs/1406.4729">SPP (Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition)</a>, ASPP and RFB</li>
      <li>YOLOv4 は SPP を採用</li>
    </ul>
  </li>
</ul>

</section>
<section>

<ul>
  <li>Attention modules for CNNs
    <ul>
      <li>channel wise attention: Squeeze-and-Excitation (SE)</li>
      <li>spatial-wise attention, like Spatial Attention Module (SAM)</li>
    </ul>
  </li>
  <li>活性化関数
    <ul>
      <li>Mish acrivation, LReLU, PReLU and ReLU6</li>
    </ul>
  </li>
</ul>

</section>
<section>

<h4>attention modules for CNNs</h4>

<ul>
  <li>channel wise attention: Squeeze-and-Excitation (SE)
    <ul>
      <li>10% 推論時間が伸びる (on GPU)</li>
    </ul>
  </li>
  <li>spatial-wise attention, like Spatial Attention Module (SAM)
    <ul>
      <li>SAM は SE ほど推論コストの悪化は多くはないらしい</li>
      <li>YOLOv4 は SAM を採用（修正あり）</li>
    </ul>
  </li>
</ul>

</section>
<section>

<h4>Activation</h4>

<ul>
  <li>
<a href="https://arxiv.org/abs/1908.08681">Mish</a>
    <ul>
      <li>Squeeze Excite Network with Mish (on CIFAR-100 dataset) resulted in an increase in Top-1 test accuracy by 0.494% and 1.671% as compared to the same network with Swish and ReLU respectively.</li>
    </ul>
  </li>
  <li><a href="https://www.desmos.com/calculator/rhx5tl8ygi">参考: Activation Functions の比較</a></li>
</ul>

</section>
<section>

<h3>Additional improvements</h3>

<ul>
  <li>Mosaic, a new data augmentation method</li>
  <li>Self-Adversarial Training (SAT)</li>
  <li>Genetic algorithmを使ったハイパーパラメータのチューニング</li>
  <li>modified SAM, modified PAN, and Cross mini-Batch Normalization (CmBN)</li>
</ul>

<aside class="notes">

<p>Mosaic と SAT は著者らによって新しく提案された Data Augmentation の手法</p>

</aside>

</section>
<section>

<h4>Mosaic</h4>

<p>4 つの画像を混ぜる</p>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-22-52-21.png" width="70%" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<h4>SAT: Self-Adversarial Training (SAT)</h4>

<ul>
  <li>1度、network weights の代わりには元々の画像を更新 (self-adversarial attack)</li>
  <li>2度目、この修正された画像に対して通常の学習を行う</li>
</ul>

</section>
<section>

<h4>Modified SAM</h4>

<p>spatial-wise から pointwise attention へ修正</p>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-17-57.png" width="70%" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<h3>Modified PAN</h3>

<p><a href="https://arxiv.org/pdf/1803.01534.pdf">PAN (Path Aggregation Network for Instance Segmentation)</a></p>

<p>addition から concatenation に変更</p>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-24-12.png" width="70%" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<h4>Cross mini-Batch Normalization (CmBN)</h4>

<p><a href="https://arxiv.org/abs/2002.05712">Cross-Iteration Batch Normalization (CBN) (2020/02/13 on arXiv)</a> をベースにして、改良を加えた</p>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-03-47.png" width="70%" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<ul>
  <li>
<a href="https://arxiv.org/abs/2002.05712">CBN (Cross-Iteration Batch Normalization)</a>
    <ul>
      <li>batch size が小さいときは、Batch Normalization の有効性が低いことが知られている</li>
      <li>CBN では複数の iteration の examples を結合することで有効性を上げる</li>
    </ul>
  </li>
  <li>CmBN は、1つの batch に含まれる全ての mini-batches のみを結合して normalize する</li>
</ul>

</section>
</section>

<section>
<section>

<h3>検証結果</h3>

</section>
<section>

<h4>Influence of different features on Classifier training</h4>

<ul>
  <li>data augmentation
    <ul>
      <li>bilateral blurring, MixUp, CutMix and Mosaic</li>
    </ul>
  </li>
  <li>activations
    <ul>
      <li>Leaky-ReLU (as default), Swish, and Mish</li>
    </ul>
  </li>
</ul>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-47-04.png" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<ul>
  <li>
<a href="https://arxiv.org/abs/1908.08681">Mish (A Self Regularized Non-Monotonic Neural Activation Function)</a> は、昨年発表された連続な活性化関数 (2019年10月)</li>
</ul>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-26-00-04-10.png" width="40%" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-48-51.png" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-25-23-50-34.png" style="background:white; border:none; box-shadow:none;"></p>

<p>CutMix, Mosaic, Label Smoothing, Mish の効果が大きい</p>

</section>
<section>

<h4>Influence of different features on Detector training</h4>

<ul>
  <li>Loss algorithms for bounded box regression
    <ul>
      <li>GIoU, CIoU, DIoU, MSE</li>
    </ul>
  </li>
  <li>単純な bbox の MSE 誤差よりも、IoU ベースの損失関数 (<a href="https://arxiv.org/abs/1911.08287">CIoU-loss: Nov 2019</a>) の方が良い</li>
</ul>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-26-00-14-20.png" style="background:white; border:none; box-shadow:none;"></p>

</section>
</section>

<section>
<section>

<h3>最終的に採用された手法</h3>

</section>
<section>

<h4>YOLOv4 アーキテクチャ</h4>

<ul>
  <li>Backbone: CSPDarknet53</li>
  <li>Neck: SPP , PAN</li>
  <li>Head: YOLOv3</li>
</ul>

</section>
<section>

<h4>Bag of Freebies (BoF) for backbone</h4>

<p>(BoF: 学習時の手法)</p>

<ul>
  <li>CutMix and Mosaic data augmentation</li>
  <li>DropBlock regularization</li>
  <li>Class label smoothing</li>
</ul>

</section>
<section>

<h4>Bag of Specials (BoS) for backbone</h4>

<p>(BoS: 推論時のテクニック・追加モジュール)</p>

<ul>
  <li>Mish activation</li>
  <li>Cross-stage partial connections (CSP)</li>
  <li>Multiinput weighted residual connections (MiWRC)</li>
</ul>

</section>
<section>

<h4>Bag of Freebies (BoF) for detector</h4>

<ul>
  <li>CIoU-loss</li>
  <li>CmBN</li>
  <li>DropBlock regularization</li>
  <li>Mosaic data augmentation</li>
  <li>Self-Adversarial Training</li>
  <li>Eliminate grid sensitivity</li>
  <li>Using multiple anchors for a single ground<br>
truth</li>
  <li>Cosine annealing scheduler</li>
  <li>Optimal hyperparameters (genetic algorithm)</li>
  <li>Random training shapes</li>
</ul>

</section>
<section>

<h4>Bag of Specials (BoS) for detector</h4>

<ul>
  <li>Mish activation</li>
  <li>SPP-block</li>
  <li>SAM-block</li>
  <li>PAN path-aggregation block</li>
  <li>DIoU-NMS</li>
</ul>

</section>
</section>

<section>
<section>

<h2>議論、課題など</h2>

<ul>
  <li>Detector の BoF を改善できる余地がある?</li>
</ul>

<blockquote>
  <p>In the future we plan to expand significantly the content of Bag of Freebies (BoF) for the detector, which theoretically can address some problems and increase the detector accuracy, and sequentially check</p>
</blockquote>

</section>
<section>

<ul>
  <li>FPSが70~130程度あるが、V100の強めのマシンであることには注意が必要（既存のモデルに対するパフォーマンスがよいことには変わりない）</li>
</ul>

</section>
<section>

<p>エッジ領域である、Jetson AGX XavierはFPSが割と小さく、TensorRTに変換及び量子化等の対応が必要</p>

<p>Jetson AGX Xavier上で32FPS程度</p>

<p>Ref: <a href="https://github.com/AlexeyAB/darknet/issues/5386#issuecomment-621169646">https://github.com/AlexeyAB/darknet/issues/5386#issuecomment-621169646</a>)</p>

</section>
<section>

<p>ちなみに</p>

<ul>
  <li>NVIDIA Tesla V100
    <ul>
      <li><a href="https://www.monotaro.com/p/3201/6094/?utm_medium=cpc&amp;utm_source=Adwords&amp;utm_campaign=246-833-4061_6466659573&amp;utm_content=96539050923&amp;utm_term=_419857551521__aud-368712506548:pla-879931900035&amp;gclid=CjwKCAjwk6P2BRAIEiwAfVJ0rI_BDVoK7CUtr7mubZ5uS0cs-s8fLxzahnQYFKn_7w2sdZ3LkJb0fxoCd0AQAvD_BwE">販売価格(税別): ¥2,990,000</a></li>
    </ul>
  </li>
</ul>

</section>
</section>

<section>
<section>

<h2>先行研究と比べて何がすごい？（新規性について）</h2>

<ul>
  <li>アーキテクチャ・手法が object detection 性能に与える影響を調査した</li>
  <li>既存の手法よりも良い手法を提案した (YOLOv4)
    <ul>
      <li>YOLOv3 と同程度に速く、より高い精度</li>
      <li>EfficientDet より速く、同程度の精度</li>
    </ul>
  </li>
  <li>速度重視で物体認識モデルを考えるのであれば、選択の筆頭候補ということになる</li>
</ul>

</section>
</section>

<section>
<section>

<h2>参考資料</h2>

<ul>
  <li>yolov4 の日本語解説資料
    <ul>
      <li><a href="https://www.slideshare.net/DeepLearningJP2016/dlyolov4-optimal-speed-and-accuracy-of-object-detection-234027228">[DL輪読会]YOLOv4: Optimal Speed and Accuracy of Object Detection</a></li>
      <li><a href="https://blog.seishin55.com/entry/2020/05/16/183132">物体認識モデルYOLOv3を軽く凌駕するYOLOv4の紹介 - ほろ酔い開発日誌</a></li>
    </ul>
  </li>
  <li><a href="https://towardsdatascience.com/yolo-v4-optimal-speed-accuracy-for-object-detection-79896ed47b50">英語解説記事</a></li>
  <li>yolov3 architecture: <a href="https://www.blog.dmprof.com/post/a-closer-look-at-yolov3">A Closer Look at YOLOv3</a>
</li>
</ul>

</section>
</section>
