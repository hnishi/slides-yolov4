<section>

<h1>YOLOv4</h1>

<p>YOLOv4: Optimal Speed and Accuracy of Object Detection</p>

<ul>
  <li>Date: 2020-04-26</li>
  <li>Speaker: Hiroshi Nishigami</li>
  <li>Links:
    <ul>
      <li>pdf: <a href="https://arxiv.org/pdf/2004.10934.pdf">https://arxiv.org/pdf/2004.10934.pdf</a></li>
      <li>abs: <a href="https://arxiv.org/abs/2004.10934">https://arxiv.org/abs/2004.10934</a></li>
      <li>github: <a href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></li>
    </ul>
  </li>
</ul>

</section>
<section>

<h2>はじめに</h2>

<ul>
  <li>yolov3 の著者 pjreddie (Joseph Redmon) が出した論文ではない</li>
  <li>pjreddie は CV の研究から引退。
    <ul>
      <li><a href="https://twitter.com/pjreddie/status/1230524770350817280?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1253524202528731136&amp;ref_url=https%3A%2F%2Fblog.seishin55.com%2Fentry%2F2020%2F05%2F16%2F183132">軍事利用やプライバシーの問題を無視できなくなったからだとか。</a></li>
    </ul>
  </li>
  <li>
<a href="https://github.com/AlexeyAB/darknet">この論文 yolov4 の first author は、 darknet を fork して開発を続けていた人。</a>
    <ul>
      <li><a href="https://twitter.com/alexeyab84/status/1264188352271613952">本人のコメント</a></li>
    </ul>
  </li>
</ul>

</section>
<section>

<h2>おさらい</h2>

<ul>
  <li>
<a href="https://pjreddie.com/darknet/yolo/">YOLO</a> とは
    <ul>
      <li>You only look once (YOLO): <a href="https://medium.com/@rajansharma9467/yolo-you-only-look-once-2afdba1f6a32">it looks for the image/frame only once and able to detect all the objects in the image/frame</a>
</li>
      <li>リアルタイム物体検出 (object detection) システム</li>
      <li>昨今の、object detection のスタンダードな手法</li>
    </ul>
  </li>
</ul>

</section>
<section>

<ul>
  <li>
<a href="https://pjreddie.com/darknet/">darknet</a> とは
    <ul>
      <li>C で書かれたオープンソースのニューラルネットワークのフレームワーク</li>
      <li>構築済み (学習済み) の yolo series が利用できる</li>
    </ul>
  </li>
</ul>

</section>
<section>

<h2>概要</h2>

<ul>
  <li>最先端のテクニック・手法を (ある程度の仮説を立てながら) 総当たりで実験し、良いものを採用するための実験を行った (for single GPU)</li>
  <li>性能が良かった組み合わせを採用して、YOLOv4 として提案</li>
  <li>既存の高速(高FPS)のアルゴリズムの中で、最も精度が良い手法</li>
  <li>YOLOv3 よりも精度が高く、EfficientDet よりも速い</li>
</ul>

</section>
<section>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-24-01-30-56.png" style="background:none; border:none; box-shadow:none;"></p>

</section>
<section>

<h3>AP: mean average precision (mAP)</h3>

<ul>
  <li><a href="http://cocodataset.org/#detection-eval">averaged across all 10 IoU thresholds and all 80 categories</a></li>
</ul>

</section>
<section>

<h2>検証内容</h2>

<ol>
  <li>Influence of different features on Classifier training</li>
  <li>Influence of different features on Detector training</li>
  <li>Influence of different backbones and pretrained weightings on Detector training</li>
  <li>Influence of different mini-batch size on Detector training</li>
</ol>

</section>
<section>

<h2>手法探索とチューニング</h2>

<ul>
  <li>モデルアーキテクチャ
    <ul>
      <li>backbone: 画像の特徴抽出の役割</li>
      <li>neck: backboneから受けた特徴マップをよしなに操作して、よりよい特徴量を生み出す</li>
      <li>head: クラス分類やbbox(物体を囲む四角形)の位置を予測する</li>
    </ul>
  </li>
  <li>Bag of freebies
    <ul>
      <li>学習上の工夫</li>
    </ul>
  </li>
  <li>Bag of specials
    <ul>
      <li>少ないコスト(推論時間や計算リソース)で大きな精度向上ができるもの</li>
    </ul>
  </li>
</ul>

</section>
<section>

<h3>物体検出器のアーキテクチャ</h3>

<p><img src="https://raw.githubusercontent.com/hnishi/slides-yolov4/master/attachments/2020-05-23-20-45-48.png" style="background:white; border:none; box-shadow:none;"></p>

</section>
<section>

<ul>
  <li>backbone: 画像の特徴抽出の役割</li>
  <li>neck: backboneから受けた特徴マップをよしなに操作して、よりよい特徴量を生み出す</li>
  <li>head: クラス分類やbbox(物体を囲む四角形)の位置を予測する
    <ul>
      <li>1-stage: 直接的に予測を行う
        <ul>
          <li>YOLO系列やSSD</li>
          <li>速度重視</li>
        </ul>
      </li>
      <li>2-stage: 候補領域を出してから予測を行う
        <ul>
          <li>R-CNN系列</li>
          <li>精度重視</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

</section>
<section>

<h3>ネットワークアーキテクチャ</h3>

<p>Backbone: CSPDarknet53<br>
Neck: SPP、PAN<br>
Head: YOLOv3</p>

<p>backbone: 画像の特徴抽出の役割<br>
neck: backboneから受けた特徴マップをよしなに操作して、よりよい特徴量を生み出します<br>
head: クラス分類やbbox(物体を囲む四角形)の位置を予測する</p>

</section>
<section>

<h3>学習上の工夫 (Bag of freebies)と精度改善上の工夫 (Bag of specials)</h3>

<ul>
  <li>活性化関数
    <ul>
      <li>Mish acrivation</li>
      <li>bboxのregression loss</li>
      <li>CIoU-loss</li>
      <li>DIoU-NMS</li>
    </ul>
  </li>
  <li>データオーグメンテーション
    <ul>
      <li>CutMix</li>
      <li>Mosaic data augmentation</li>
      <li>Self-Adversarial Training</li>
    </ul>
  </li>
  <li>正則化
    <ul>
      <li>DropBlock regularization</li>
    </ul>
  </li>
  <li>正規化
    <ul>
      <li>CmBN</li>
    </ul>
  </li>
  <li>その他
    <ul>
      <li>Optimal hyper parameters</li>
      <li>Cosine annealing scheduler</li>
      <li>Class label smoothing</li>
    </ul>
  </li>
</ul>

</section>
<section>

<h3>最終的に採用された手法</h3>

<ul>
  <li>Head: YOLOv3のHeadを採用
    <ul>
      <li>Anchor-basedな手法</li>
    </ul>
  </li>
  <li>Bag of Freebies(BoF, 学習時の手法)
    <ul>
      <li>Backbone: CutMix, Mosaic data augmentation, DropBlock regularization, class label smoothing</li>
      <li>Detector(head): CIoU-loss, CmBN, DropBlock regularization, Mosaic data augmentation, self-adversarial training, Eliminate grid sensitivity, Using multiple anchors for a single ground truth, Cosine annealing scheduler, optimal hyperparameters, random training shape</li>
    </ul>
  </li>
  <li>Bag of Specials(BoS, 推論時のテクニック・追加モジュール)
    <ul>
      <li>Backbone: Mish activation, Cross-stage partial connection, Multi input weighted residual connection</li>
      <li>Detector(head): Mish activation, SPP-block(improved, 前述), SAM-block(improved), Pan path-aggregation block, DIoU-NMS 24 Methodology</li>
    </ul>
  </li>
</ul>

</section>
<section>

<h2>議論、課題など</h2>

<ul>
  <li>detector の BoF を改善できる余地がある（future work）　</li>
  <li>FPSが70~130程度あるが、V100の強めのマシンであることには注意が必要（既存のモデルに対するパフォーマンスがよいことには変わりない）</li>
</ul>

</section>
<section>

<p>エッジ領域である、Jetson AGX XavierはFPSが割と小さく、TensorRTに変換及び量子化等の対応が必要</p>

<p>Jetson AGX Xavier上で32FPS程度</p>

<p>Ref: <a href="https://github.com/AlexeyAB/darknet/issues/5386#issuecomment-621169646">https://github.com/AlexeyAB/darknet/issues/5386#issuecomment-621169646</a>)</p>

</section>
<section>

<p>ちなみに</p>

<ul>
  <li>NVIDIA Tesla V100
    <ul>
      <li><a href="https://www.monotaro.com/p/3201/6094/?utm_medium=cpc&amp;utm_source=Adwords&amp;utm_campaign=246-833-4061_6466659573&amp;utm_content=96539050923&amp;utm_term=_419857551521__aud-368712506548:pla-879931900035&amp;gclid=CjwKCAjwk6P2BRAIEiwAfVJ0rI_BDVoK7CUtr7mubZ5uS0cs-s8fLxzahnQYFKn_7w2sdZ3LkJb0fxoCd0AQAvD_BwE">販売価格(税別): ¥2,990,000</a></li>
    </ul>
  </li>
</ul>

</section>
<section>

<h2>先行研究と比べて何がすごい？（新規性について）</h2>

<ul>
  <li>アーキテクチャ・手法が object detection 性能に与える影響を調査した</li>
  <li>既存の手法よりも良い手法を提案した (YOLOv4)
    <ul>
      <li>YOLOv3 と同程度に速く、より高い精度</li>
      <li>EfficientDet より速く、同程度の精度</li>
    </ul>
  </li>
  <li>速度重視で物体認識モデルを考えるのであれば、選択の筆頭候補ということになる</li>
</ul>

</section>
<section>

<h2>関連する論文</h2>

<ul>
  <li>network
    <ul>
      <li>
<a href="https://arxiv.org/pdf/1911.11929.pdf">CSPNet</a>
        <ul>
          <li>精度をあまり落とさずに、計算コストを省略するための手法</li>
        </ul>
      </li>
      <li><a href="https://arxiv.org/abs/1406.4729">SPP (Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition)</a></li>
      <li><a href="https://arxiv.org/pdf/1803.01534.pdf">PAN (Path Aggregation Network for Instance Segmentation)</a></li>
    </ul>
  </li>
  <li>Mish</li>
</ul>

</section>
<section>

<h2>参考資料</h2>

<ul>
  <li>yolov4 の日本語解説資料
    <ul>
      <li><a href="https://www.slideshare.net/DeepLearningJP2016/dlyolov4-optimal-speed-and-accuracy-of-object-detection-234027228">[DL輪読会]YOLOv4: Optimal Speed and Accuracy of Object Detection</a></li>
      <li><a href="https://blog.seishin55.com/entry/2020/05/16/183132">物体認識モデルYOLOv3を軽く凌駕するYOLOv4の紹介 - ほろ酔い開発日誌</a></li>
    </ul>
  </li>
  <li><a href="https://towardsdatascience.com/yolo-v4-optimal-speed-accuracy-for-object-detection-79896ed47b50">英語解説記事</a></li>
  <li>yolov3 architecture: <a href="https://www.blog.dmprof.com/post/a-closer-look-at-yolov3">A Closer Look at YOLOv3</a>
</li>
</ul>

</section>
